{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_text_generation_Char-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObaFfZ2mKMa3i1E9QYZntH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woodstone10/deep_learning_study-python/blob/main/NLP_text_generation_Char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQAxOgLnoPt"
      },
      "source": [
        "# Natural Language Processing - **Text generation using character RNN**\n",
        "\n",
        "Reference:\n",
        "- Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
        "- https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cP0fpLC6rHC"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sklearn.preprocessing"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ut6TosC1ul"
      },
      "source": [
        "**Data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ISr-BL7J7e"
      },
      "source": [
        "First, let’s download all of Shakespeare’s work, using Keras’s handy get_file() function and downloading the data from Andrej Karpathy’s Char-RNN project : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PhmH09Mgyrf"
      },
      "source": [
        "url = \"https://homl.info/shakespeare\" "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUZNKqhZlDSu"
      },
      "source": [
        "import urllib\n",
        "f = urllib.request.urlopen(url)\n",
        "html = f.read()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJiwVaEtnVgy",
        "outputId": "250053d8-bb5a-46b1-de10-4a439ce358d0"
      },
      "source": [
        "print(html[:250])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMBmu8c5lIXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337a91e3-ce1a-44d5-9e34-54eb464d2acc"
      },
      "source": [
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", url) \n",
        "with open(filepath) as f: \n",
        "  shakespeare_text = f.read()\n",
        "print(len(shakespeare_text))\n",
        "print(shakespeare_text[:250]) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3PsElfONN2h"
      },
      "source": [
        "Save (writing) shakespeare_text.txt file on Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkJNGkzCEbdc",
        "outputId": "2bfe95dd-7e07-4f1b-8de8-931313e86b6d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aidmdp7xEl2F"
      },
      "source": [
        "f = open(\"/content/drive/My Drive/My Colab/deep_learning_study-python/shakespeare_text.txt\", \"a\")\n",
        "f.write(shakespeare_text)\n",
        "f.close()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRj5wmP6Nayb"
      },
      "source": [
        "Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSnHcGPlnJAS",
        "outputId": "cfcbda98-101a-4d73-8f83-b0e851557c63"
      },
      "source": [
        "vocab = sorted(set(shakespeare_text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ntbT0VFoC7XB",
        "outputId": "20517b72-d19f-4d47-f149-4e73c71ad1a9"
      },
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NnqM8Ic8kYf"
      },
      "source": [
        "we must encode every character as an integer. One option is to create a custom preprocessing layer. But in this case, it will be simpler to use Keras’s Tokenizer class. First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0prRYUdx7_HJ"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text) "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL42mYsG-LoH"
      },
      "source": [
        "Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbF4Nupq9Njr",
        "outputId": "1d596262-1410-4b68-c1b8-de7515d75b54"
      },
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi5Ss5yU9f56",
        "outputId": "48a6e27f-0e4e-495a-a872-c08efc8e4813"
      },
      "source": [
        "tokenizer.sequences_to_texts([[ 20 , 6 , 9 , 8 , 3 ]]) \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO83vtXKAIGQ"
      },
      "source": [
        "number of distinct characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nm0B6dA9noy",
        "outputId": "7623efda-a41b-4ddb-a451-fef27ff9dfb0"
      },
      "source": [
        "max_id = len(tokenizer.word_index) \n",
        "max_id"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aphgWZ9hAB8r"
      },
      "source": [
        "total number of characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWyM04vo94v3",
        "outputId": "35fd3c4d-e420-42a6-d5bb-1e46f982e0cb"
      },
      "source": [
        "dataset_size = tokenizer.document_count \n",
        "dataset_size"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oVa5OTX-y4T"
      },
      "source": [
        "Let’s encode the full text so each character is represented by its ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQFZTwI4-iJR",
        "outputId": "90ad8fa6-77bf-46d0-d1da-003b52f97e0c"
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 \n",
        "encoded"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19,  5,  8, ..., 20, 26, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXStdqQsRwEd"
      },
      "source": [
        "the first 90% of the text for the training set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wVTGBTeCKlg",
        "outputId": "c6f48a79-b19b-45a8-b891-bcb630ff7ad9"
      },
      "source": [
        "train_size = dataset_size * 90 // 100 \n",
        "train_size"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXZW8bxBUYu3",
        "outputId": "f394b1de-444a-446e-d0cc-f0c9a677c3f6"
      },
      "source": [
        "int(dataset_size * 0.9)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd9BolW3RBk4"
      },
      "source": [
        "create a tf.data.Dataset that will return each character one by one from this set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNwAl3laRUoR"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices( encoded[:train_size] ) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW0x2VTBVIRp"
      },
      "source": [
        "**Chopping the Sequential Dataset into Multiple Windows **\n",
        "\n",
        "The training set now consists of a single sequence of over a million characters. we would have a single (very long) instance to train it. Instead, we will use the dataset’s window() method to convert this long sequence of characters into many smaller windows of text. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRhuLM8QDh2S"
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-jrQ-guKiuj"
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOXQbZ_eDptd",
        "outputId": "802e5912-513c-4deb-c1e3-76688b1f0874"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "  print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGL1pkwe9I54"
      },
      "source": [
        "**Model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ6-wm1mB5PB"
      },
      "source": [
        "Please use GPU for faster learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2gq-jll-IB3",
        "outputId": "50f1e009-51c9-48cd-9d71-ba327f82cb17"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwbUrbFmCG3Y"
      },
      "source": [
        "GRU class will only use the GPU (if you have one) when using the default values for the following arguments: activation, recurrent_activation, recurrent_dropout, unroll, use_bias and reset_after. This is why I commented out recurrent_dropout=0.2 (compared to the book)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHo4lU8G9K1X",
        "outputId": "40c7938e-8e08-499d-be76-d0fb63c15b4e"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.GRU(128, return_sequences=True, \n",
        "                      input_shape=[None, max_id],\n",
        "                      dropout=0.2),\n",
        "  tf.keras.layers.GRU(128, return_sequences=True,\n",
        "                      dropout=0.2),\n",
        "  tf.keras.layers.TimeDistributed(\n",
        "      tf.keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
        "              optimizer=\"adam\")\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(dataset, \n",
        "                      steps_per_epoch=train_size // batch_size,\n",
        "                      epochs=3)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "31370/31370 [==============================] - 376s 12ms/step - loss: 1.7189\n",
            "Epoch 2/3\n",
            "31370/31370 [==============================] - 374s 12ms/step - loss: 1.5442\n",
            "Epoch 3/3\n",
            "31370/31370 [==============================] - 370s 12ms/step - loss: 1.5200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8ToA39NDFyA"
      },
      "source": [
        "**Test**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiKrIQgw_9Rx"
      },
      "source": [
        "def preprocess(texts):\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AF8cveZiADcj",
        "outputId": "5c12cfff-247b-4538-8cd0-eb87d1b7ea44"
      },
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dysRxLWOMJWl"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_84BFao1MNVz",
        "outputId": "f5c6e545-3d0d-460c-c84f-ed97352fdbae"
      },
      "source": [
        "next_char(\"How are yo\", temperature=1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zd70aB1cMRQp",
        "outputId": "08dcdc9b-6592-4903-dc10-f05fca57ab0d"
      },
      "source": [
        "y= next_char(\"How are yo\", temperature=1)\n",
        "y"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA01XAIAMXpW"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e817nqIUMbj2",
        "outputId": "b682056b-cb1f-43d8-d2e0-261fa4caa1b2"
      },
      "source": [
        "print(complete_text(\"how are you doing\", temperature=1))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how are you doing for a senuty.\n",
            "it comes him to great all the eseme\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}